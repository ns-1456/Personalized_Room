{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Fine-Tuning for SDXL Inpainting Model\n",
        "\n",
        "This notebook fine-tunes a Stable Diffusion XL inpainting model using QLoRA (Quantized LoRA) for efficient training on Google Colab.\n",
        "\n",
        "## Dataset Format Required:\n",
        "- `input_image` or `image`: PIL.Image - The input room image\n",
        "- `mask`: PIL.Image (grayscale) - Mask indicating where to edit\n",
        "- `edit_instruction` or `instruction` or `text`: str - Text instruction describing the edit\n",
        "- `output_image` or `edited_image`: PIL.Image - The expected output image after editing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q torch diffusers transformers peft bitsandbytes accelerate datasets Pillow numpy safetensors xformers\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive (Optional)\n",
        "\n",
        "If you want to save your trained adapter to Google Drive, mount it here. Otherwise, the adapter will be saved in the Colab runtime (temporary).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (uncomment if you want to save to Drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set output directory (change this if you mounted Drive)\n",
        "OUTPUT_DIR = \"./my-room-editor-qlora\"\n",
        "# OUTPUT_DIR = \"/content/drive/MyDrive/my-room-editor-qlora\"  # Use this if Drive is mounted\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your training parameters here. Adjust these based on your dataset size and available GPU memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and dataset configuration\n",
        "MODEL_NAME = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
        "DATASET_NAME = \"fusing/instructpix2pix-1000-samples\"  # Replace with your dataset\n",
        "VAE_ID = \"madebyollin/sdxl-vae-fp16-fix\"\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_TRAIN_EPOCHS = 5\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MIXED_PRECISION = \"fp16\"\n",
        "RESOLUTION = 1024\n",
        "LORA_RANK = 4\n",
        "LORA_ALPHA = 4\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Dataset: {DATASET_NAME}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_TRAIN_EPOCHS}\")\n",
        "print(f\"  Batch size: {TRAIN_BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  LoRA rank: {LORA_RANK}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    UNet2DConditionModel,\n",
        "    DDPMScheduler,\n",
        ")\n",
        "from diffusers.optimization import get_scheduler\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import transformers\n",
        "import diffusers\n",
        "import datasets\n",
        "\n",
        "# Setup logging\n",
        "logger = get_logger(__name__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Dataset Structure\n",
        "\n",
        "First, let's load the dataset and check what keys it has. This will help us understand the dataset format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset to inspect its structure\n",
        "try:\n",
        "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "    print(f\"Dataset loaded! Size: {len(dataset)}\")\n",
        "    \n",
        "    # Check the first example to see what keys are available\n",
        "    if len(dataset) > 0:\n",
        "        first_example = dataset[0]\n",
        "        print(\"\\nAvailable keys in dataset:\")\n",
        "        print(list(first_example.keys()))\n",
        "        print(\"\\nFirst example structure:\")\n",
        "        for key, value in first_example.items():\n",
        "            if isinstance(value, Image.Image):\n",
        "                print(f\"  {key}: PIL.Image ({value.size}, {value.mode})\")\n",
        "            elif isinstance(value, str):\n",
        "                print(f\"  {key}: str (length: {len(value)})\")\n",
        "            else:\n",
        "                print(f\"  {key}: {type(value).__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "    print(\"Please check your dataset name and format.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class and Helper Functions\n",
        "\n",
        "The dataset class now supports multiple possible key names to handle different dataset formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InpaintingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for inpainting fine-tuning.\n",
        "    \n",
        "    Supports multiple key name formats:\n",
        "    - input_image, image, input\n",
        "    - output_image, edited_image, output, edited\n",
        "    - edit_instruction, instruction, text, prompt\n",
        "    - mask (optional)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        tokenizer,\n",
        "        vae,\n",
        "        size=1024,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vae = vae\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def _get_key(self, example, possible_keys, default=None):\n",
        "        \"\"\"Try multiple possible key names.\"\"\"\n",
        "        for key in possible_keys:\n",
        "            if key in example:\n",
        "                return example[key]\n",
        "        if default is not None:\n",
        "            return default\n",
        "        raise KeyError(f\"Could not find any of these keys: {possible_keys}. Available keys: {list(example.keys())}\")\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "        \n",
        "        # Try multiple possible key names for input image\n",
        "        input_image = self._get_key(\n",
        "            example,\n",
        "            [\"input_image\", \"image\", \"input\", \"original_image\"],\n",
        "        )\n",
        "        \n",
        "        # Try multiple possible key names for output image\n",
        "        output_image = self._get_key(\n",
        "            example,\n",
        "            [\"output_image\", \"edited_image\", \"output\", \"edited\", \"target_image\"],\n",
        "        )\n",
        "        \n",
        "        # Convert to PIL Image if needed\n",
        "        if isinstance(input_image, dict):\n",
        "            input_image = Image.open(input_image[\"path\"]) if \"path\" in input_image else input_image\n",
        "        if not isinstance(input_image, Image.Image):\n",
        "            if isinstance(input_image, np.ndarray):\n",
        "                input_image = Image.fromarray(input_image)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected input_image type: {type(input_image)}\")\n",
        "        \n",
        "        if isinstance(output_image, dict):\n",
        "            output_image = Image.open(output_image[\"path\"]) if \"path\" in output_image else output_image\n",
        "        if not isinstance(output_image, Image.Image):\n",
        "            if isinstance(output_image, np.ndarray):\n",
        "                output_image = Image.fromarray(output_image)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected output_image type: {type(output_image)}\")\n",
        "        \n",
        "        # Load mask (optional - will create default if not present)\n",
        "        mask = None\n",
        "        for mask_key in [\"mask\", \"mask_image\", \"edit_mask\"]:\n",
        "            if mask_key in example:\n",
        "                mask = example[mask_key]\n",
        "                break\n",
        "        \n",
        "        if mask is None:\n",
        "            # If no mask provided, create a simple mask (you may want to generate this differently)\n",
        "            mask = Image.new(\"L\", input_image.size, 255)\n",
        "        else:\n",
        "            if isinstance(mask, dict):\n",
        "                mask = Image.open(mask[\"path\"]) if \"path\" in mask else mask\n",
        "            if not isinstance(mask, Image.Image):\n",
        "                if isinstance(mask, np.ndarray):\n",
        "                    mask = Image.fromarray(mask)\n",
        "                else:\n",
        "                    mask = Image.new(\"L\", input_image.size, 255)\n",
        "            if mask.mode != \"L\":\n",
        "                mask = mask.convert(\"L\")\n",
        "        \n",
        "        # Get instruction text - try multiple possible keys\n",
        "        instruction = self._get_key(\n",
        "            example,\n",
        "            [\"edit_instruction\", \"instruction\", \"text\", \"prompt\", \"edit_prompt\", \"caption\"],\n",
        "            default=\"\",  # Default to empty string if not found\n",
        "        )\n",
        "        \n",
        "        # Resize and preprocess images\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        output_image = output_image.convert(\"RGB\")\n",
        "        \n",
        "        # Resize maintaining aspect ratio\n",
        "        def resize_image(image, size):\n",
        "            image.thumbnail((size, size), Image.Resampling.LANCZOS)\n",
        "            new_image = Image.new(\"RGB\", (size, size), (0, 0, 0))\n",
        "            new_image.paste(image, ((size - image.width) // 2, (size - image.height) // 2))\n",
        "            return new_image\n",
        "        \n",
        "        input_image = resize_image(input_image, self.size)\n",
        "        output_image = resize_image(output_image, self.size)\n",
        "        mask = mask.resize((self.size, self.size), Image.Resampling.LANCZOS)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        input_image = np.array(input_image).astype(np.float32) / 255.0\n",
        "        output_image = np.array(output_image).astype(np.float32) / 255.0\n",
        "        mask = np.array(mask).astype(np.float32) / 255.0\n",
        "        \n",
        "        # Normalize to [-1, 1]\n",
        "        input_image = (input_image - 0.5) / 0.5\n",
        "        output_image = (output_image - 0.5) / 0.5\n",
        "        \n",
        "        # Expand mask to 3 channels\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        \n",
        "        return {\n",
        "            \"input_image\": torch.from_numpy(input_image).permute(2, 0, 1),\n",
        "            \"output_image\": torch.from_numpy(output_image).permute(2, 0, 1),\n",
        "            \"mask\": torch.from_numpy(mask),\n",
        "            \"instruction\": instruction,\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"Collate function for batching.\"\"\"\n",
        "    input_images = [example[\"input_image\"] for example in examples]\n",
        "    output_images = [example[\"output_image\"] for example in examples]\n",
        "    masks = [example[\"mask\"] for example in examples]\n",
        "    instructions = [example[\"instruction\"] for example in examples]\n",
        "    \n",
        "    # Stack tensors\n",
        "    input_images = torch.stack(input_images)\n",
        "    output_images = torch.stack(output_images)\n",
        "    masks = torch.stack(masks)\n",
        "    \n",
        "    return {\n",
        "        \"input_images\": input_images,\n",
        "        \"output_images\": output_images,\n",
        "        \"masks\": masks,\n",
        "        \"instructions\": instructions,\n",
        "    }\n",
        "\n",
        "\n",
        "def encode_prompt(text_encoder, text_encoder_2, tokenizer, tokenizer_2, prompts, device):\n",
        "    \"\"\"Encode prompts using both text encoders (SDXL uses dual encoders).\"\"\"\n",
        "    # Tokenize\n",
        "    tokenizer_output = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=77,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = tokenizer_output.input_ids.to(device)\n",
        "    \n",
        "    tokenizer_2_output = tokenizer_2(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=77,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids_2 = tokenizer_2_output.input_ids.to(device)\n",
        "    \n",
        "    # Encode\n",
        "    with torch.no_grad():\n",
        "        prompt_embeds = text_encoder(input_ids)[0]\n",
        "        prompt_embeds_2 = text_encoder_2(input_ids_2)[0]\n",
        "    \n",
        "    # Concatenate embeddings\n",
        "    prompt_embeds = torch.cat([prompt_embeds, prompt_embeds_2], dim=-1)\n",
        "    \n",
        "    # Get pooled embeddings\n",
        "    pooled_prompt_embeds = text_encoder_2(input_ids_2)[0].mean(dim=1)\n",
        "    \n",
        "    return prompt_embeds, pooled_prompt_embeds\n",
        "\n",
        "\n",
        "def get_time_ids(batch_size, device):\n",
        "    \"\"\"Get time IDs for SDXL conditioning.\"\"\"\n",
        "    # SDXL uses time_ids for size and crop conditioning\n",
        "    # Format: [original_size, crops_coords_top_left, target_size]\n",
        "    # For simplicity, we use default values (1024x1024, no crop)\n",
        "    time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], dtype=torch.float32, device=device)\n",
        "    return time_ids.repeat(batch_size, 1)\n",
        "\n",
        "print(\"Dataset class and helper functions defined!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Accelerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup accelerator\n",
        "logging_dir = Path(OUTPUT_DIR, \"logs\")\n",
        "project_config = ProjectConfiguration(project_dir=OUTPUT_DIR, logging_dir=logging_dir)\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    mixed_precision=MIXED_PRECISION,\n",
        "    project_config=project_config,\n",
        ")\n",
        "\n",
        "# Configure logging\n",
        "if accelerator.is_local_main_process:\n",
        "    datasets.utils.logging.set_verbosity_warning()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "    diffusers.utils.logging.set_verbosity_info()\n",
        "else:\n",
        "    datasets.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    diffusers.utils.logging.set_verbosity_error()\n",
        "\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "print(\"Accelerator setup complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer and text encoders\n",
        "logger.info(\"Loading tokenizer and text encoders...\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\")\n",
        "tokenizer_2 = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer_2\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
        "text_encoder_2 = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder_2\", torch_dtype=torch.float16)\n",
        "\n",
        "# Load VAE\n",
        "logger.info(\"Loading VAE...\")\n",
        "vae = AutoencoderKL.from_pretrained(VAE_ID, torch_dtype=torch.float16)\n",
        "vae.requires_grad_(False)\n",
        "vae.eval()\n",
        "\n",
        "# Load UNet\n",
        "logger.info(\"Loading UNet...\")\n",
        "unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\", torch_dtype=torch.float16)\n",
        "\n",
        "# Enable memory-efficient attention (xformers) if available\n",
        "try:\n",
        "    unet.enable_xformers_memory_efficient_attention()\n",
        "    print(\"âœ“ XFormers memory-efficient attention enabled!\")\n",
        "except Exception as e:\n",
        "    print(f\"XFormers not available: {e}\")\n",
        "    # Fallback to attention slicing\n",
        "    try:\n",
        "        unet.enable_attention_slicing(slice_size=\"max\")\n",
        "        print(\"âœ“ Attention slicing enabled as fallback!\")\n",
        "    except Exception as e2:\n",
        "        print(f\"Could not enable attention optimizations: {e2}\")\n",
        "\n",
        "# Enable VAE tiling for memory efficiency\n",
        "try:\n",
        "    vae.enable_tiling()\n",
        "    print(\"âœ“ VAE tiling enabled for memory efficiency!\")\n",
        "except Exception as e:\n",
        "    print(f\"VAE tiling not available: {e}\")\n",
        "\n",
        "# Freeze text encoders and VAE\n",
        "text_encoder.requires_grad_(False)\n",
        "text_encoder_2.requires_grad_(False)\n",
        "text_encoder.eval()\n",
        "text_encoder_2.eval()\n",
        "\n",
        "print(\"Models loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "logger.info(\"Configuring LoRA adapters...\")\n",
        "unet_lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        ")\n",
        "\n",
        "# Apply LoRA to UNet\n",
        "unet = get_peft_model(unet, unet_lora_config)\n",
        "unet.print_trainable_parameters()\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency\n",
        "# This allows training with larger effective batch sizes\n",
        "if hasattr(unet, \"enable_gradient_checkpointing\"):\n",
        "    unet.enable_gradient_checkpointing()\n",
        "    print(\"Gradient checkpointing enabled!\")\n",
        "\n",
        "# Compile UNet for faster training (PyTorch 2.0+)\n",
        "# This can provide 20-30% speedup\n",
        "try:\n",
        "    if hasattr(torch, \"compile\"):\n",
        "        print(\"Compiling UNet with torch.compile()...\")\n",
        "        unet = torch.compile(unet, mode=\"reduce-overhead\")\n",
        "        print(\"âœ“ UNet compiled successfully!\")\n",
        "    else:\n",
        "        print(\"torch.compile not available (requires PyTorch 2.0+)\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not compile UNet: {e}\")\n",
        "    print(\"Continuing without compilation...\")\n",
        "\n",
        "# Load noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
        "print(\"LoRA adapters configured!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Prepare Dataset\n",
        "\n",
        "Now we'll reload the dataset and create the training dataloader. The dataset class will automatically handle different key formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "logger.info(f\"Loading dataset: {DATASET_NAME}\")\n",
        "try:\n",
        "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "    print(f\"Dataset loaded! Size: {len(dataset)}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Could not load dataset {DATASET_NAME}: {e}\")\n",
        "    logger.info(\"Please ensure your dataset has the required format:\")\n",
        "    logger.info(\"  - input_image or image: PIL.Image\")\n",
        "    logger.info(\"  - output_image or edited_image: PIL.Image\")\n",
        "    logger.info(\"  - mask (optional): PIL.Image (grayscale)\")\n",
        "    logger.info(\"  - edit_instruction or instruction or text: str\")\n",
        "    raise\n",
        "\n",
        "# Create dataset wrapper\n",
        "train_dataset = InpaintingDataset(\n",
        "    dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    vae=vae,\n",
        "    size=RESOLUTION,\n",
        ")\n",
        "\n",
        "# Test the dataset with the first example to catch any errors early\n",
        "try:\n",
        "    test_sample = train_dataset[0]\n",
        "    print(\"âœ“ Dataset test successful! Sample keys:\", list(test_sample.keys()))\n",
        "except Exception as e:\n",
        "    print(f\"âœ— Error testing dataset: {e}\")\n",
        "    print(\"\\nPlease check the dataset format. The dataset should have:\")\n",
        "    print(\"  - An input image (key: 'input_image', 'image', or 'input')\")\n",
        "    print(\"  - An output image (key: 'output_image', 'edited_image', or 'output')\")\n",
        "    print(\"  - An instruction text (key: 'edit_instruction', 'instruction', or 'text')\")\n",
        "    print(\"  - Optionally a mask (key: 'mask')\")\n",
        "    raise\n",
        "\n",
        "# Create optimized dataloader\n",
        "# Use multiple workers for faster data loading\n",
        "# pin_memory=True speeds up GPU transfer\n",
        "# prefetch_factor helps pipeline data loading\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,  # 2-4 workers for Colab (adjust based on available CPU)\n",
        "    pin_memory=True,  # Faster GPU transfer\n",
        "    prefetch_factor=2,  # Prefetch batches for smoother pipeline\n",
        "    persistent_workers=True,  # Keep workers alive between epochs\n",
        ")\n",
        "\n",
        "print(f\"Dataset prepared! Training samples: {len(train_dataset)}\")\n",
        "print(\"âœ“ DataLoader optimized with num_workers=2, pin_memory=True, prefetch_factor=2\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Optimizer and Learning Rate Scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    unet.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-2,\n",
        "    eps=1e-08,\n",
        ")\n",
        "\n",
        "# Calculate number of training steps\n",
        "num_update_steps_per_epoch = len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS\n",
        "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
        "max_train_steps = NUM_TRAIN_EPOCHS * num_update_steps_per_epoch\n",
        "\n",
        "# Setup learning rate scheduler\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"constant\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=max_train_steps,\n",
        ")\n",
        "\n",
        "# Prepare everything with accelerator\n",
        "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    unet, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "# Move VAE and text encoders to device\n",
        "vae = vae.to(accelerator.device)\n",
        "text_encoder = text_encoder.to(accelerator.device)\n",
        "text_encoder_2 = text_encoder_2.to(accelerator.device)\n",
        "\n",
        "print(f\"Training setup complete!\")\n",
        "print(f\"  Total training steps: {max_train_steps}\")\n",
        "print(f\"  Steps per epoch: {num_update_steps_per_epoch}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "This is the main training loop. It will train for the specified number of epochs and save checkpoints after each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "logger.info(\"Starting training...\")\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(NUM_TRAIN_EPOCHS):\n",
        "    unet.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    progress_bar = tqdm(\n",
        "        total=num_update_steps_per_epoch,  # Use update steps, not batches\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "        desc=f\"Epoch {epoch + 1}/{NUM_TRAIN_EPOCHS}\",\n",
        "    )\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "            with accelerator.accumulate(unet):\n",
        "                # Move data to device with non_blocking for faster transfer\n",
        "                input_images = batch[\"input_images\"].to(accelerator.device, dtype=torch.float16, non_blocking=True)\n",
        "                output_images = batch[\"output_images\"].to(accelerator.device, dtype=torch.float16, non_blocking=True)\n",
        "                masks = batch[\"masks\"].to(accelerator.device, dtype=torch.float16, non_blocking=True)\n",
        "                \n",
        "                # Encode images to latents (both in one no_grad block for efficiency)\n",
        "                with torch.no_grad():\n",
        "                    # Encode both images in parallel if possible\n",
        "                    input_latents = vae.encode(input_images).latent_dist.sample() * vae.config.scaling_factor\n",
        "                    output_latents = vae.encode(output_images).latent_dist.sample() * vae.config.scaling_factor\n",
        "                \n",
        "                    # Prepare mask for latents\n",
        "                # Resize mask to match latent space dimensions\n",
        "                # Mask shape: [batch, 1, H, W] where 1 = edit area, 0 = keep area\n",
        "                mask_tensor = F.interpolate(\n",
        "                    masks,\n",
        "                    size=(input_latents.shape[2], input_latents.shape[3]),\n",
        "                    mode=\"nearest\",\n",
        "                )\n",
        "                # Ensure mask is in [0, 1] range (should already be from dataset preprocessing)\n",
        "                mask_tensor = torch.clamp(mask_tensor, 0.0, 1.0)\n",
        "                \n",
        "                # Get batch size\n",
        "                bsz = output_latents.shape[0]\n",
        "                \n",
        "                # Encode text prompts\n",
        "                instructions = batch[\"instructions\"]\n",
        "                with torch.no_grad():\n",
        "                    # Tokenize with both tokenizers\n",
        "                    prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
        "                        text_encoder, text_encoder_2, tokenizer, tokenizer_2, instructions, accelerator.device\n",
        "                    )\n",
        "                    time_ids = get_time_ids(bsz, accelerator.device)\n",
        "                \n",
        "                # Sample noise\n",
        "                noise = torch.randn_like(output_latents)\n",
        "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=output_latents.device)\n",
        "                timesteps = timesteps.long()\n",
        "                \n",
        "                # Add noise to latents\n",
        "                noisy_latents = noise_scheduler.add_noise(output_latents, noise, timesteps)\n",
        "                \n",
        "                # Prepare input for inpainting: SDXL inpainting expects 9 channels\n",
        "                # Concatenate: [input_latents (4), mask (1), noisy_latents (4)] = 9 channels\n",
        "                # The mask should be in the range [0, 1] where 1 means \"edit this area\"\n",
        "                mask_normalized = mask_tensor  # Already in [0, 1] range\n",
        "                model_input = torch.cat([input_latents, mask_normalized, noisy_latents], dim=1)\n",
        "                \n",
        "                # Predict noise with autocast for better mixed precision performance\n",
        "                with torch.cuda.amp.autocast(enabled=MIXED_PRECISION == \"fp16\"):\n",
        "                    model_pred = unet(\n",
        "                        model_input,\n",
        "                        timesteps,\n",
        "                        encoder_hidden_states=prompt_embeds,\n",
        "                        added_cond_kwargs={\"text_embeds\": pooled_prompt_embeds, \"time_ids\": time_ids},\n",
        "                    ).sample\n",
        "                \n",
        "                # Compute loss (use float32 for stability)\n",
        "                loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "                \n",
        "                # Backward pass\n",
        "                accelerator.backward(loss)\n",
        "                if accelerator.sync_gradients:\n",
        "                    accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "            \n",
        "            if accelerator.sync_gradients:\n",
        "                progress_bar.update(1)\n",
        "                global_step += 1\n",
        "                train_loss += loss.detach().item()\n",
        "                \n",
        "                # Reduce logging frequency to every 200 steps (was 100) for less overhead\n",
        "                if global_step % 200 == 0:\n",
        "                    avg_loss = train_loss / 200\n",
        "                    # Use print instead of logger for faster output\n",
        "                    progress_bar.set_postfix({\"loss\": f\"{avg_loss:.4f}\"})\n",
        "                    if accelerator.is_local_main_process:\n",
        "                        print(f\"\\nStep {global_step}, Loss: {avg_loss:.4f}\")\n",
        "                    train_loss = 0.0\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    # Save checkpoint after each epoch\n",
        "    if accelerator.is_main_process:\n",
        "        logger.info(f\"Saving checkpoint after epoch {epoch + 1}...\")\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"checkpoint-{epoch + 1}\")\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        # Save using PEFT's save_pretrained which works with diffusers\n",
        "        unet.save_pretrained(save_path)\n",
        "        logger.info(f\"Checkpoint saved to {save_path}\")\n",
        "        print(f\"âœ“ Checkpoint saved: {save_path}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Final Adapter\n",
        "\n",
        "Save the final trained adapter weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final adapter\n",
        "if accelerator.is_main_process:\n",
        "    logger.info(\"Saving final adapter...\")\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    # Save the PEFT model - this will create adapter_model.safetensors and adapter_config.json\n",
        "    # which can be loaded by the pipeline using load_lora_weights()\n",
        "    unet.save_pretrained(OUTPUT_DIR)\n",
        "    logger.info(f\"Final adapter saved to {OUTPUT_DIR}\")\n",
        "    print(f\"âœ“ Final adapter saved to: {OUTPUT_DIR}\")\n",
        "    print(f\"Adapter can be loaded in app.py using: pipeline.load_lora_weights('{OUTPUT_DIR}')\")\n",
        "    \n",
        "    # List saved files\n",
        "    print(\"\\nSaved files:\")\n",
        "    for file in os.listdir(OUTPUT_DIR):\n",
        "        file_path = os.path.join(OUTPUT_DIR, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"  - {file} ({size:.2f} MB)\")\n",
        "\n",
        "accelerator.end_training()\n",
        "print(\"\\nðŸŽ‰ Training finished successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Adapter (if not using Google Drive)\n",
        "\n",
        "If you saved to the Colab runtime (not Drive), download the adapter files before the session ends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download adapter files (uncomment if needed)\n",
        "# from google.colab import files\n",
        "# import shutil\n",
        "\n",
        "# # Create a zip file of the adapter\n",
        "# shutil.make_archive(\"my-room-editor-qlora\", \"zip\", OUTPUT_DIR)\n",
        "# files.download(\"my-room-editor-qlora.zip\")\n",
        "\n",
        "print(\"To download the adapter, uncomment the code above or copy from Google Drive if you mounted it.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA Fine-Tuning for SDXL Inpainting Model\n",
        "\n",
        "This notebook fine-tunes a Stable Diffusion XL inpainting model using QLoRA (Quantized LoRA) for efficient training on Google Colab.\n",
        "\n",
        "## Dataset Format Required:\n",
        "- `input_image`: PIL.Image - The input room image\n",
        "- `mask`: PIL.Image (grayscale) - Mask indicating where to edit\n",
        "- `edit_instruction`: str - Text instruction describing the edit\n",
        "- `output_image`: PIL.Image - The expected output image after editing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q torch diffusers transformers peft bitsandbytes accelerate datasets Pillow numpy safetensors xformers\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mount Google Drive (Optional)\n",
        "\n",
        "If you want to save your trained adapter to Google Drive, mount it here. Otherwise, the adapter will be saved in the Colab runtime (temporary).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (uncomment if you want to save to Drive)\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set output directory (change this if you mounted Drive)\n",
        "OUTPUT_DIR = \"./my-room-editor-qlora\"\n",
        "# OUTPUT_DIR = \"/content/drive/MyDrive/my-room-editor-qlora\"  # Use this if Drive is mounted\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set your training parameters here. Adjust these based on your dataset size and available GPU memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model and dataset configuration\n",
        "MODEL_NAME = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
        "DATASET_NAME = \"fusing/instructpix2pix-1000-samples\"  # Replace with your dataset\n",
        "VAE_ID = \"madebyollin/sdxl-vae-fp16-fix\"\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_TRAIN_EPOCHS = 5\n",
        "TRAIN_BATCH_SIZE = 1\n",
        "GRADIENT_ACCUMULATION_STEPS = 4\n",
        "MIXED_PRECISION = \"fp16\"\n",
        "RESOLUTION = 1024\n",
        "LORA_RANK = 4\n",
        "LORA_ALPHA = 4\n",
        "LORA_DROPOUT = 0.1\n",
        "\n",
        "print(\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Dataset: {DATASET_NAME}\")\n",
        "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {NUM_TRAIN_EPOCHS}\")\n",
        "print(f\"  Batch size: {TRAIN_BATCH_SIZE}\")\n",
        "print(f\"  Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
        "print(f\"  LoRA rank: {LORA_RANK}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    UNet2DConditionModel,\n",
        "    DDPMScheduler,\n",
        ")\n",
        "from diffusers.optimization import get_scheduler\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import ProjectConfiguration\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import transformers\n",
        "import diffusers\n",
        "import datasets\n",
        "\n",
        "# Setup logging\n",
        "logger = get_logger(__name__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Class and Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InpaintingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class for inpainting fine-tuning.\n",
        "    \n",
        "    Expects dataset with keys: 'input_image', 'mask', 'edit_instruction', 'output_image'\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset,\n",
        "        tokenizer,\n",
        "        vae,\n",
        "        size=1024,\n",
        "        center_crop=False,\n",
        "    ):\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vae = vae\n",
        "        self.size = size\n",
        "        self.center_crop = center_crop\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "        \n",
        "        # Load images\n",
        "        input_image = example[\"input_image\"]\n",
        "        if isinstance(input_image, dict):\n",
        "            input_image = Image.open(input_image[\"path\"]) if \"path\" in input_image else input_image\n",
        "        if not isinstance(input_image, Image.Image):\n",
        "            input_image = Image.fromarray(input_image)\n",
        "        \n",
        "        output_image = example[\"output_image\"]\n",
        "        if isinstance(output_image, dict):\n",
        "            output_image = Image.open(output_image[\"path\"]) if \"path\" in output_image else output_image\n",
        "        if not isinstance(output_image, Image.Image):\n",
        "            output_image = Image.fromarray(output_image)\n",
        "        \n",
        "        # Load mask\n",
        "        mask = example.get(\"mask\", None)\n",
        "        if mask is None:\n",
        "            # If no mask provided, create a simple mask (you may want to generate this differently)\n",
        "            mask = Image.new(\"L\", input_image.size, 255)\n",
        "        else:\n",
        "            if isinstance(mask, dict):\n",
        "                mask = Image.open(mask[\"path\"]) if \"path\" in mask else mask\n",
        "            if not isinstance(mask, Image.Image):\n",
        "                mask = Image.fromarray(mask)\n",
        "            if mask.mode != \"L\":\n",
        "                mask = mask.convert(\"L\")\n",
        "        \n",
        "        # Get instruction text\n",
        "        instruction = example.get(\"edit_instruction\", \"\")\n",
        "        if not instruction:\n",
        "            instruction = example.get(\"instruction\", \"\")\n",
        "        if not instruction:\n",
        "            instruction = example.get(\"text\", \"\")\n",
        "        \n",
        "        # Resize and preprocess images\n",
        "        input_image = input_image.convert(\"RGB\")\n",
        "        output_image = output_image.convert(\"RGB\")\n",
        "        \n",
        "        # Resize maintaining aspect ratio\n",
        "        def resize_image(image, size):\n",
        "            image.thumbnail((size, size), Image.Resampling.LANCZOS)\n",
        "            new_image = Image.new(\"RGB\", (size, size), (0, 0, 0))\n",
        "            new_image.paste(image, ((size - image.width) // 2, (size - image.height) // 2))\n",
        "            return new_image\n",
        "        \n",
        "        input_image = resize_image(input_image, self.size)\n",
        "        output_image = resize_image(output_image, self.size)\n",
        "        mask = mask.resize((self.size, self.size), Image.Resampling.LANCZOS)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        input_image = np.array(input_image).astype(np.float32) / 255.0\n",
        "        output_image = np.array(output_image).astype(np.float32) / 255.0\n",
        "        mask = np.array(mask).astype(np.float32) / 255.0\n",
        "        \n",
        "        # Normalize to [-1, 1]\n",
        "        input_image = (input_image - 0.5) / 0.5\n",
        "        output_image = (output_image - 0.5) / 0.5\n",
        "        \n",
        "        # Expand mask to 3 channels\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "        \n",
        "        return {\n",
        "            \"input_image\": torch.from_numpy(input_image).permute(2, 0, 1),\n",
        "            \"output_image\": torch.from_numpy(output_image).permute(2, 0, 1),\n",
        "            \"mask\": torch.from_numpy(mask),\n",
        "            \"instruction\": instruction,\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    \"\"\"Collate function for batching.\"\"\"\n",
        "    input_images = [example[\"input_image\"] for example in examples]\n",
        "    output_images = [example[\"output_image\"] for example in examples]\n",
        "    masks = [example[\"mask\"] for example in examples]\n",
        "    instructions = [example[\"instruction\"] for example in examples]\n",
        "    \n",
        "    # Stack tensors\n",
        "    input_images = torch.stack(input_images)\n",
        "    output_images = torch.stack(output_images)\n",
        "    masks = torch.stack(masks)\n",
        "    \n",
        "    return {\n",
        "        \"input_images\": input_images,\n",
        "        \"output_images\": output_images,\n",
        "        \"masks\": masks,\n",
        "        \"instructions\": instructions,\n",
        "    }\n",
        "\n",
        "\n",
        "def encode_prompt(text_encoder, text_encoder_2, tokenizer, tokenizer_2, prompts, device):\n",
        "    \"\"\"Encode prompts using both text encoders (SDXL uses dual encoders).\"\"\"\n",
        "    # Tokenize\n",
        "    tokenizer_output = tokenizer(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=77,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = tokenizer_output.input_ids.to(device)\n",
        "    \n",
        "    tokenizer_2_output = tokenizer_2(\n",
        "        prompts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=77,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids_2 = tokenizer_2_output.input_ids.to(device)\n",
        "    \n",
        "    # Encode\n",
        "    with torch.no_grad():\n",
        "        prompt_embeds = text_encoder(input_ids)[0]\n",
        "        prompt_embeds_2 = text_encoder_2(input_ids_2)[0]\n",
        "    \n",
        "    # Concatenate embeddings\n",
        "    prompt_embeds = torch.cat([prompt_embeds, prompt_embeds_2], dim=-1)\n",
        "    \n",
        "    # Get pooled embeddings\n",
        "    pooled_prompt_embeds = text_encoder_2(input_ids_2)[0].mean(dim=1)\n",
        "    \n",
        "    return prompt_embeds, pooled_prompt_embeds\n",
        "\n",
        "\n",
        "def get_time_ids(batch_size, device):\n",
        "    \"\"\"Get time IDs for SDXL conditioning.\"\"\"\n",
        "    # SDXL uses time_ids for size and crop conditioning\n",
        "    # Format: [original_size, crops_coords_top_left, target_size]\n",
        "    # For simplicity, we use default values (1024x1024, no crop)\n",
        "    time_ids = torch.tensor([[1024, 1024, 0, 0, 1024, 1024]], dtype=torch.float32, device=device)\n",
        "    return time_ids.repeat(batch_size, 1)\n",
        "\n",
        "print(\"Dataset class and helper functions defined!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Accelerator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup accelerator\n",
        "logging_dir = Path(OUTPUT_DIR, \"logs\")\n",
        "project_config = ProjectConfiguration(project_dir=OUTPUT_DIR, logging_dir=logging_dir)\n",
        "accelerator = Accelerator(\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    mixed_precision=MIXED_PRECISION,\n",
        "    project_config=project_config,\n",
        ")\n",
        "\n",
        "# Configure logging\n",
        "if accelerator.is_local_main_process:\n",
        "    datasets.utils.logging.set_verbosity_warning()\n",
        "    transformers.utils.logging.set_verbosity_warning()\n",
        "    diffusers.utils.logging.set_verbosity_info()\n",
        "else:\n",
        "    datasets.utils.logging.set_verbosity_error()\n",
        "    transformers.utils.logging.set_verbosity_error()\n",
        "    diffusers.utils.logging.set_verbosity_error()\n",
        "\n",
        "logger.info(accelerator.state, main_process_only=False)\n",
        "print(\"Accelerator setup complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer and text encoders\n",
        "logger.info(\"Loading tokenizer and text encoders...\")\n",
        "tokenizer = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer\")\n",
        "tokenizer_2 = CLIPTokenizer.from_pretrained(MODEL_NAME, subfolder=\"tokenizer_2\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
        "text_encoder_2 = CLIPTextModel.from_pretrained(MODEL_NAME, subfolder=\"text_encoder_2\", torch_dtype=torch.float16)\n",
        "\n",
        "# Load VAE\n",
        "logger.info(\"Loading VAE...\")\n",
        "vae = AutoencoderKL.from_pretrained(VAE_ID, torch_dtype=torch.float16)\n",
        "vae.requires_grad_(False)\n",
        "vae.eval()\n",
        "\n",
        "# Load UNet\n",
        "logger.info(\"Loading UNet...\")\n",
        "unet = UNet2DConditionModel.from_pretrained(MODEL_NAME, subfolder=\"unet\", torch_dtype=torch.float16)\n",
        "\n",
        "# Freeze text encoders and VAE\n",
        "text_encoder.requires_grad_(False)\n",
        "text_encoder_2.requires_grad_(False)\n",
        "text_encoder.eval()\n",
        "text_encoder_2.eval()\n",
        "\n",
        "print(\"Models loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LoRA Adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure LoRA\n",
        "logger.info(\"Configuring LoRA adapters...\")\n",
        "unet_lora_config = LoraConfig(\n",
        "    r=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    init_lora_weights=\"gaussian\",\n",
        "    target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        ")\n",
        "\n",
        "# Apply LoRA to UNet\n",
        "unet = get_peft_model(unet, unet_lora_config)\n",
        "unet.print_trainable_parameters()\n",
        "\n",
        "# Load noise scheduler\n",
        "noise_scheduler = DDPMScheduler.from_pretrained(MODEL_NAME, subfolder=\"scheduler\")\n",
        "print(\"LoRA adapters configured!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "logger.info(f\"Loading dataset: {DATASET_NAME}\")\n",
        "try:\n",
        "    dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
        "    print(f\"Dataset loaded! Size: {len(dataset)}\")\n",
        "except Exception as e:\n",
        "    logger.warning(f\"Could not load dataset {DATASET_NAME}: {e}\")\n",
        "    logger.info(\"Please ensure your dataset has the required format:\")\n",
        "    logger.info(\"  - input_image: PIL.Image\")\n",
        "    logger.info(\"  - mask: PIL.Image (grayscale)\")\n",
        "    logger.info(\"  - edit_instruction: str\")\n",
        "    logger.info(\"  - output_image: PIL.Image\")\n",
        "    raise\n",
        "\n",
        "# Create dataset wrapper\n",
        "train_dataset = InpaintingDataset(\n",
        "    dataset=dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    vae=vae,\n",
        "    size=RESOLUTION,\n",
        ")\n",
        "\n",
        "# Create dataloader\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=2,  # Reduced for Colab\n",
        ")\n",
        "\n",
        "print(f\"Dataset prepared! Training samples: {len(train_dataset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Optimizer and Learning Rate Scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer\n",
        "optimizer = torch.optim.AdamW(\n",
        "    unet.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    betas=(0.9, 0.999),\n",
        "    weight_decay=1e-2,\n",
        "    eps=1e-08,\n",
        ")\n",
        "\n",
        "# Calculate number of training steps\n",
        "num_update_steps_per_epoch = len(train_dataloader) // GRADIENT_ACCUMULATION_STEPS\n",
        "num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n",
        "max_train_steps = NUM_TRAIN_EPOCHS * num_update_steps_per_epoch\n",
        "\n",
        "# Setup learning rate scheduler\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"constant\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=max_train_steps,\n",
        ")\n",
        "\n",
        "# Prepare everything with accelerator\n",
        "unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "    unet, optimizer, train_dataloader, lr_scheduler\n",
        ")\n",
        "\n",
        "# Move VAE and text encoders to device\n",
        "vae = vae.to(accelerator.device)\n",
        "text_encoder = text_encoder.to(accelerator.device)\n",
        "text_encoder_2 = text_encoder_2.to(accelerator.device)\n",
        "\n",
        "print(f\"Training setup complete!\")\n",
        "print(f\"  Total training steps: {max_train_steps}\")\n",
        "print(f\"  Steps per epoch: {num_update_steps_per_epoch}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "\n",
        "This is the main training loop. It will train for the specified number of epochs and save checkpoints after each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "logger.info(\"Starting training...\")\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(NUM_TRAIN_EPOCHS):\n",
        "    unet.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    progress_bar = tqdm(\n",
        "        total=len(train_dataloader),\n",
        "        disable=not accelerator.is_local_main_process,\n",
        "        desc=f\"Epoch {epoch + 1}/{NUM_TRAIN_EPOCHS}\",\n",
        "    )\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        with accelerator.accumulate(unet):\n",
        "            # Encode images to latents\n",
        "            input_images = batch[\"input_images\"].to(dtype=torch.float16)\n",
        "            output_images = batch[\"output_images\"].to(dtype=torch.float16)\n",
        "            masks = batch[\"masks\"].to(dtype=torch.float16)\n",
        "            \n",
        "            # Encode input image to latents\n",
        "            with torch.no_grad():\n",
        "                input_latents = vae.encode(input_images).latent_dist.sample()\n",
        "                input_latents = input_latents * vae.config.scaling_factor\n",
        "                \n",
        "                # Encode output image to latents\n",
        "                output_latents = vae.encode(output_images).latent_dist.sample()\n",
        "                output_latents = output_latents * vae.config.scaling_factor\n",
        "            \n",
        "            # Prepare mask for latents\n",
        "            mask_tensor = F.interpolate(\n",
        "                masks,\n",
        "                size=(input_latents.shape[2], input_latents.shape[3]),\n",
        "                mode=\"nearest\",\n",
        "            )\n",
        "            \n",
        "            # Get batch size\n",
        "            bsz = output_latents.shape[0]\n",
        "            \n",
        "            # Encode text prompts\n",
        "            instructions = batch[\"instructions\"]\n",
        "            with torch.no_grad():\n",
        "                # Tokenize with both tokenizers\n",
        "                prompt_embeds, pooled_prompt_embeds = encode_prompt(\n",
        "                    text_encoder, text_encoder_2, tokenizer, tokenizer_2, instructions, accelerator.device\n",
        "                )\n",
        "                time_ids = get_time_ids(bsz, accelerator.device)\n",
        "            \n",
        "            # Sample noise\n",
        "            noise = torch.randn_like(output_latents)\n",
        "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=output_latents.device)\n",
        "            timesteps = timesteps.long()\n",
        "            \n",
        "            # Add noise to latents\n",
        "            noisy_latents = noise_scheduler.add_noise(output_latents, noise, timesteps)\n",
        "            \n",
        "            # Prepare input for inpainting: combine masked input latents with noisy output latents\n",
        "            masked_input_latents = input_latents * (1 - mask_tensor)\n",
        "            noisy_masked_output_latents = noisy_latents * mask_tensor\n",
        "            model_input = masked_input_latents + noisy_masked_output_latents\n",
        "            \n",
        "            # Predict noise\n",
        "            model_pred = unet(\n",
        "                model_input,\n",
        "                timesteps,\n",
        "                encoder_hidden_states=prompt_embeds,\n",
        "                added_cond_kwargs={\"text_embeds\": pooled_prompt_embeds, \"time_ids\": time_ids},\n",
        "            ).sample\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n",
        "            \n",
        "            # Backward pass\n",
        "            accelerator.backward(loss)\n",
        "            if accelerator.sync_gradients:\n",
        "                accelerator.clip_grad_norm_(unet.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        if accelerator.sync_gradients:\n",
        "            progress_bar.update(1)\n",
        "            global_step += 1\n",
        "            train_loss += loss.detach().item()\n",
        "            \n",
        "            if global_step % 100 == 0:\n",
        "                avg_loss = train_loss / 100\n",
        "                logger.info(f\"Step {global_step}, Loss: {avg_loss:.4f}\")\n",
        "                train_loss = 0.0\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    # Save checkpoint after each epoch\n",
        "    if accelerator.is_main_process:\n",
        "        logger.info(f\"Saving checkpoint after epoch {epoch + 1}...\")\n",
        "        save_path = os.path.join(OUTPUT_DIR, f\"checkpoint-{epoch + 1}\")\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        # Save using PEFT's save_pretrained which works with diffusers\n",
        "        unet.save_pretrained(save_path)\n",
        "        logger.info(f\"Checkpoint saved to {save_path}\")\n",
        "        print(f\"âœ“ Checkpoint saved: {save_path}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Final Adapter\n",
        "\n",
        "Save the final trained adapter weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final adapter\n",
        "if accelerator.is_main_process:\n",
        "    logger.info(\"Saving final adapter...\")\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    # Save the PEFT model - this will create adapter_model.safetensors and adapter_config.json\n",
        "    # which can be loaded by the pipeline using load_lora_weights()\n",
        "    unet.save_pretrained(OUTPUT_DIR)\n",
        "    logger.info(f\"Final adapter saved to {OUTPUT_DIR}\")\n",
        "    print(f\"âœ“ Final adapter saved to: {OUTPUT_DIR}\")\n",
        "    print(f\"Adapter can be loaded in app.py using: pipeline.load_lora_weights('{OUTPUT_DIR}')\")\n",
        "    \n",
        "    # List saved files\n",
        "    print(\"\\nSaved files:\")\n",
        "    for file in os.listdir(OUTPUT_DIR):\n",
        "        file_path = os.path.join(OUTPUT_DIR, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
        "            print(f\"  - {file} ({size:.2f} MB)\")\n",
        "\n",
        "accelerator.end_training()\n",
        "print(\"\\nðŸŽ‰ Training finished successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Adapter (if not using Google Drive)\n",
        "\n",
        "If you saved to the Colab runtime (not Drive), download the adapter files before the session ends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download adapter files (uncomment if needed)\n",
        "# from google.colab import files\n",
        "# import shutil\n",
        "\n",
        "# # Create a zip file of the adapter\n",
        "# shutil.make_archive(\"my-room-editor-qlora\", \"zip\", OUTPUT_DIR)\n",
        "# files.download(\"my-room-editor-qlora.zip\")\n",
        "\n",
        "print(\"To download the adapter, uncomment the code above or copy from Google Drive if you mounted it.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
