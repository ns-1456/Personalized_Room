{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¨ Personalize Your Room AI - Google Colab Version\n",
        "\n",
        "**Memory-Optimized for Colab's Limited RAM**\n",
        "\n",
        "This version:\n",
        "- Loads models sequentially (only one at a time)\n",
        "- Uses GPU with aggressive memory optimizations\n",
        "- Unloads models when switching between generation and editing\n",
        "- Reduces RAM usage from ~12GB to ~6-8GB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q gradio torch diffusers transformers peft accelerate bitsandbytes xformers Pillow numpy safetensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Mount Google Drive & Verify Fine-Tuned Adapter\n",
        "\n",
        "**Important:** This step mounts your Google Drive so the app can access your trained QLoRA adapter. The fine-tuned adapter is what makes the model follow your specific instructions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to your adapter (adjust if needed)\n",
        "# This should point to the folder containing your trained QLoRA adapter\n",
        "# The folder should contain: adapter_config.json and adapter_model.safetensors (or .bin)\n",
        "ADAPTER_PATH = \"/content/drive/My Drive/SFT-interesting ideas/Personalized_Room/my-room-editor-qlora\"\n",
        "\n",
        "# Verify adapter exists\n",
        "from pathlib import Path\n",
        "adapter_path = Path(ADAPTER_PATH)\n",
        "if adapter_path.exists():\n",
        "    print(f\"âœ“ Found adapter at: {ADAPTER_PATH}\")\n",
        "    # List files in adapter directory\n",
        "    adapter_files = list(adapter_path.glob(\"*\"))\n",
        "    print(f\"  Files in adapter folder: {[f.name for f in adapter_files]}\")\n",
        "else:\n",
        "    print(f\"âš  WARNING: Adapter not found at: {ADAPTER_PATH}\")\n",
        "    print(\"  The app will run without your fine-tuned adapter (using base model only)\")\n",
        "    print(\"  To use your fine-tuned model:\")\n",
        "    print(\"  1. Make sure your adapter folder is in Google Drive\")\n",
        "    print(\"  2. Update ADAPTER_PATH above to the correct location\")\n",
        "    print(\"  3. Or upload the adapter folder to Colab's /content directory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import Libraries & Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from diffusers import (\n",
        "    StableDiffusionXLPipeline,\n",
        "    StableDiffusionXLInpaintPipeline,\n",
        "    AutoencoderKL,\n",
        ")\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import gc\n",
        "from pathlib import Path\n",
        "\n",
        "# Check device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
        "print(f\"Using device: {DEVICE}, dtype: {DTYPE}\")\n",
        "\n",
        "# Model IDs\n",
        "BASE_GEN_MODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "EDIT_MODEL_ID = \"diffusers/stable-diffusion-xl-1.0-inpainting-0.1\"\n",
        "VAE_ID = \"madebyollin/sdxl-vae-fp16-fix\"\n",
        "\n",
        "# Path to adapter\n",
        "QLORA_ADAPTER_PATH = ADAPTER_PATH if 'ADAPTER_PATH' in globals() else \"./my-room-editor-qlora\"\n",
        "\n",
        "# Global variables - only one pipeline loaded at a time\n",
        "base_generator_pipe = None\n",
        "editor_pipe = None\n",
        "vae = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Memory-Optimized Model Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_base_generator():\n",
        "    \"\"\"Load base generator pipeline. Unloads editor if loaded.\"\"\"\n",
        "    global base_generator_pipe, editor_pipe, vae\n",
        "    \n",
        "    # Unload editor to free memory\n",
        "    if editor_pipe is not None:\n",
        "        print(\"Unloading editor pipeline to free memory...\")\n",
        "        del editor_pipe\n",
        "        editor_pipe = None\n",
        "        torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
        "        gc.collect()\n",
        "    \n",
        "    if base_generator_pipe is not None:\n",
        "        print(\"Base generator already loaded.\")\n",
        "        return True\n",
        "    \n",
        "    try:\n",
        "        print(\"Loading VAE...\")\n",
        "        if vae is None:\n",
        "            vae = AutoencoderKL.from_pretrained(\n",
        "                VAE_ID,\n",
        "                torch_dtype=DTYPE,\n",
        "            )\n",
        "        \n",
        "        print(\"Loading Base Generator Pipeline...\")\n",
        "        base_generator_pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            BASE_GEN_MODEL_ID,\n",
        "            vae=vae,\n",
        "            torch_dtype=DTYPE,\n",
        "            variant=\"fp16\" if DTYPE == torch.float16 else None,\n",
        "            use_safetensors=True,\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "        \n",
        "        # Aggressive memory optimizations\n",
        "        base_generator_pipe.enable_attention_slicing(slice_size=\"max\")\n",
        "        if DEVICE == \"cuda\":\n",
        "            try:\n",
        "                base_generator_pipe.enable_xformers_memory_efficient_attention()\n",
        "            except:\n",
        "                pass\n",
        "        base_generator_pipe.enable_vae_slicing()\n",
        "        base_generator_pipe.enable_vae_tiling()\n",
        "        \n",
        "        if DEVICE == \"cuda\":\n",
        "            base_generator_pipe = base_generator_pipe.to(DEVICE)\n",
        "        else:\n",
        "            base_generator_pipe.enable_model_cpu_offload()\n",
        "        \n",
        "        print(\"âœ“ Base generator loaded successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading base generator: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "\n",
        "def load_editor():\n",
        "    \"\"\"Load editor pipeline. Unloads base generator if loaded.\"\"\"\n",
        "    global base_generator_pipe, editor_pipe, vae\n",
        "    \n",
        "    # Unload base generator to free memory\n",
        "    if base_generator_pipe is not None:\n",
        "        print(\"Unloading base generator to free memory...\")\n",
        "        del base_generator_pipe\n",
        "        base_generator_pipe = None\n",
        "        torch.cuda.empty_cache() if DEVICE == \"cuda\" else None\n",
        "        gc.collect()\n",
        "    \n",
        "    if editor_pipe is not None:\n",
        "        print(\"Editor already loaded.\")\n",
        "        return True\n",
        "    \n",
        "    try:\n",
        "        print(\"Loading VAE...\")\n",
        "        if vae is None:\n",
        "            vae = AutoencoderKL.from_pretrained(\n",
        "                VAE_ID,\n",
        "                torch_dtype=DTYPE,\n",
        "            )\n",
        "        \n",
        "        print(\"Loading Editor Pipeline...\")\n",
        "        editor_pipe = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
        "            EDIT_MODEL_ID,\n",
        "            vae=vae,\n",
        "            torch_dtype=DTYPE,\n",
        "            variant=\"fp16\" if DTYPE == torch.float16 else None,\n",
        "            use_safetensors=True,\n",
        "            low_cpu_mem_usage=True,\n",
        "        )\n",
        "        \n",
        "        # Aggressive memory optimizations\n",
        "        editor_pipe.enable_attention_slicing(slice_size=\"max\")\n",
        "        if DEVICE == \"cuda\":\n",
        "            try:\n",
        "                editor_pipe.enable_xformers_memory_efficient_attention()\n",
        "            except:\n",
        "                pass\n",
        "        editor_pipe.enable_vae_slicing()\n",
        "        editor_pipe.enable_vae_tiling()\n",
        "        \n",
        "        # Load QLoRA adapter if available (THIS IS YOUR FINE-TUNED MODEL!)\n",
        "        adapter_path = Path(QLORA_ADAPTER_PATH)\n",
        "        if adapter_path.exists():\n",
        "            try:\n",
        "                print(f\"ðŸŽ¯ Loading YOUR fine-tuned QLoRA adapter from: {QLORA_ADAPTER_PATH}\")\n",
        "                editor_pipe.load_lora_weights(QLORA_ADAPTER_PATH)\n",
        "                print(\"âœ… SUCCESS: Your fine-tuned adapter is loaded! The model will follow your training.\")\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Could not load adapter: {e}\")\n",
        "                print(\"   The app will run with the base model (not fine-tuned)\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Adapter not found at: {QLORA_ADAPTER_PATH}\")\n",
        "            print(\"   Running with base model (not fine-tuned)\")\n",
        "        \n",
        "        if DEVICE == \"cuda\":\n",
        "            editor_pipe = editor_pipe.to(DEVICE)\n",
        "        else:\n",
        "            editor_pipe.enable_model_cpu_offload()\n",
        "        \n",
        "        print(\"âœ“ Editor loaded successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading editor: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generation and Editing Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_base_room(prompt: str, num_inference_steps: int = 30, guidance_scale: float = 7.5):\n",
        "    \"\"\"Generate base room image.\"\"\"\n",
        "    if not prompt or not prompt.strip():\n",
        "        raise gr.Error(\"Please provide a room description prompt.\")\n",
        "    \n",
        "    # Load base generator (will unload editor if needed)\n",
        "    if not load_base_generator():\n",
        "        raise gr.Error(\"Failed to load base generator model.\")\n",
        "    \n",
        "    try:\n",
        "        print(f\"Generating base room: {prompt}\")\n",
        "        \n",
        "        negative_prompt = (\n",
        "            \"low quality, worst quality, blurry, people, person, text, watermark, \"\n",
        "            \"deformed, distorted, disfigured, bad anatomy, bad proportions\"\n",
        "        )\n",
        "        \n",
        "        with torch.inference_mode():\n",
        "            image = base_generator_pipe(\n",
        "                prompt=prompt,\n",
        "                negative_prompt=negative_prompt,\n",
        "                num_inference_steps=int(num_inference_steps),\n",
        "                guidance_scale=float(guidance_scale),\n",
        "                height=1024,\n",
        "                width=1024,\n",
        "            ).images[0]\n",
        "        \n",
        "        print(\"âœ“ Base room generated!\")\n",
        "        return image\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise gr.Error(f\"Failed to generate room: {str(e)}\")\n",
        "\n",
        "\n",
        "def edit_room(image_editor_output: dict, instruction: str, num_inference_steps: int = 20, guidance_scale: float = 7.0):\n",
        "    \"\"\"Edit room with mask-based inpainting.\"\"\"\n",
        "    if not instruction or not instruction.strip():\n",
        "        raise gr.Error(\"Please provide an edit instruction.\")\n",
        "    \n",
        "    if image_editor_output is None:\n",
        "        raise gr.Error(\"Please generate or upload a room image first.\")\n",
        "    \n",
        "    # Load editor (will unload base generator if needed)\n",
        "    if not load_editor():\n",
        "        raise gr.Error(\"Failed to load editor model.\")\n",
        "    \n",
        "    try:\n",
        "        # Extract image and mask from ImageEditor output\n",
        "        if isinstance(image_editor_output, dict):\n",
        "            input_image = image_editor_output.get(\"background\") or image_editor_output.get(\"composite\")\n",
        "            layers = image_editor_output.get(\"layers\", [])\n",
        "            \n",
        "            if input_image is None:\n",
        "                raise gr.Error(\"Invalid image editor output.\")\n",
        "            \n",
        "            if not layers:\n",
        "                raise gr.Error(\"Please draw a mask on the image to indicate which areas to edit.\")\n",
        "            \n",
        "            # Create mask from layers\n",
        "            mask_array = np.zeros((input_image.height, input_image.width), dtype=np.uint8)\n",
        "            \n",
        "            for layer in layers:\n",
        "                if isinstance(layer, dict) and \"image\" in layer:\n",
        "                    layer_img = layer[\"image\"]\n",
        "                    if isinstance(layer_img, Image.Image):\n",
        "                        layer_array = np.array(layer_img.convert(\"RGB\"))\n",
        "                    else:\n",
        "                        layer_array = np.array(layer_img)\n",
        "                    \n",
        "                    if len(layer_array.shape) == 3:\n",
        "                        white_threshold = 200\n",
        "                        white_mask = (layer_array[:, :, 0] > white_threshold) & \\\n",
        "                                    (layer_array[:, :, 1] > white_threshold) & \\\n",
        "                                    (layer_array[:, :, 2] > white_threshold)\n",
        "                        mask_array[white_mask] = 255\n",
        "            \n",
        "            # Fallback: extract from composite if no layers\n",
        "            if mask_array.sum() == 0:\n",
        "                composite = image_editor_output.get(\"composite\")\n",
        "                if composite:\n",
        "                    composite_array = np.array(composite.convert(\"RGB\"))\n",
        "                    bg_array = np.array(input_image.convert(\"RGB\"))\n",
        "                    diff = np.abs(composite_array.astype(int) - bg_array.astype(int)).sum(axis=2)\n",
        "                    mask_array[diff > 50] = 255\n",
        "            \n",
        "            if mask_array.sum() == 0:\n",
        "                raise gr.Error(\"Please draw a mask on the image.\")\n",
        "            \n",
        "            mask_pil = Image.fromarray(mask_array, mode=\"L\")\n",
        "        else:\n",
        "            raise gr.Error(\"Please draw a mask on the image.\")\n",
        "        \n",
        "        # Resize to 1024x1024\n",
        "        original_size = input_image.size\n",
        "        image_1024 = input_image.resize((1024, 1024), Image.Resampling.LANCZOS)\n",
        "        mask_1024 = mask_pil.resize((1024, 1024), Image.Resampling.LANCZOS)\n",
        "        \n",
        "        # Binarize mask\n",
        "        mask_array = np.array(mask_1024)\n",
        "        mask_array = (mask_array > 128).astype(np.uint8) * 255\n",
        "        mask_1024 = Image.fromarray(mask_array, mode=\"L\")\n",
        "        \n",
        "        print(f\"Editing room: {instruction}\")\n",
        "        \n",
        "        # Run inpainting\n",
        "        with torch.inference_mode():\n",
        "            edited_image = editor_pipe(\n",
        "                prompt=instruction,\n",
        "                image=image_1024,\n",
        "                mask_image=mask_1024,\n",
        "                num_inference_steps=int(num_inference_steps),\n",
        "                guidance_scale=float(guidance_scale),\n",
        "                strength=0.95,\n",
        "            ).images[0]\n",
        "        \n",
        "        edited_image = edited_image.resize(original_size, Image.Resampling.LANCZOS)\n",
        "        print(\"âœ“ Room edited successfully!\")\n",
        "        return edited_image\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise gr.Error(f\"Failed to edit room: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Gradio Interface & Launch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_interface():\n",
        "    with gr.Blocks(title=\"Personalize Your Room AI - Colab\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            # ðŸŽ¨ Personalize Your Room AI (Colab Version)\n",
        "            \n",
        "            **Memory-optimized for Google Colab** - Only one model loaded at a time!\n",
        "            \n",
        "            **How to use:**\n",
        "            1. Enter a room description and generate a base image\n",
        "            2. Draw a white mask on areas you want to edit\n",
        "            3. Enter an instruction and click \"Apply Edit\"\n",
        "            \"\"\"\n",
        "        )\n",
        "        \n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### Generate Base Room\")\n",
        "                base_prompt = gr.Textbox(\n",
        "                    label=\"Room Description\",\n",
        "                    placeholder=\"A modern living room with large windows, minimalist furniture, and plants\",\n",
        "                    lines=3,\n",
        "                )\n",
        "                gen_steps = gr.Slider(label=\"Steps\", minimum=10, maximum=50, value=30, step=1)\n",
        "                gen_guidance = gr.Slider(label=\"Guidance Scale\", minimum=1.0, maximum=20.0, value=7.5, step=0.5)\n",
        "                base_button = gr.Button(\"Generate Room\", variant=\"primary\")\n",
        "                clear_base = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "                \n",
        "                gr.Markdown(\"### Edit Room\")\n",
        "                edit_instruction = gr.Textbox(\n",
        "                    label=\"Edit Instruction\",\n",
        "                    placeholder=\"Add a red sofa\",\n",
        "                    lines=2,\n",
        "                )\n",
        "                edit_steps = gr.Slider(label=\"Steps\", minimum=10, maximum=50, value=20, step=1)\n",
        "                edit_guidance = gr.Slider(label=\"Guidance Scale\", minimum=1.0, maximum=20.0, value=7.0, step=0.5)\n",
        "                edit_button = gr.Button(\"Apply Edit\", variant=\"primary\")\n",
        "                clear_edit = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "            \n",
        "            with gr.Column(scale=2):\n",
        "                gr.Markdown(\"### Your Design Canvas\")\n",
        "                image_canvas = gr.ImageEditor(\n",
        "                    label=\"Room Canvas - Draw white mask to indicate edit areas\",\n",
        "                    type=\"pil\",\n",
        "                    height=600,\n",
        "                    brush=gr.Brush(colors=[\"#FFFFFF\"], color_mode=\"fixed\"),\n",
        "                )\n",
        "                clear_canvas = gr.Button(\"Clear Canvas\", variant=\"secondary\")\n",
        "        \n",
        "        # Wire up UI\n",
        "        base_button.click(\n",
        "            fn=generate_base_room,\n",
        "            inputs=[base_prompt, gen_steps, gen_guidance],\n",
        "            outputs=[image_canvas],\n",
        "        )\n",
        "        \n",
        "        edit_button.click(\n",
        "            fn=edit_room,\n",
        "            inputs=[image_canvas, edit_instruction, edit_steps, edit_guidance],\n",
        "            outputs=[image_canvas],\n",
        "        )\n",
        "        \n",
        "        clear_base.click(lambda: \"\", outputs=[base_prompt])\n",
        "        clear_edit.click(lambda: \"\", outputs=[edit_instruction])\n",
        "        clear_canvas.click(lambda: None, outputs=[image_canvas])\n",
        "    \n",
        "    return demo\n",
        "\n",
        "\n",
        "# Create and launch interface\n",
        "demo = create_interface()\n",
        "demo.launch(share=True, server_name=\"0.0.0.0\", server_port=7860)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
